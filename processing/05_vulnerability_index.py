"""
05_vulnerability_index.py
-------------------------
Combines deprivation, shoplifting trend, crime-deprivation mismatch,
and policing intensity into a composite borough vulnerability score.
Also builds the crime trajectory summary and shoplifting scenarios
for the 'Where is London Headed?' section.

Outputs:
    data/processed/borough_vulnerability.csv
    data/processed/crime_trajectory.csv
    data/processed/shoplifting_scenarios.csv

Run from project root:
    python processing/05_vulnerability_index.py
"""

import os
import numpy as np
import pandas as pd

# ── Paths ─────────────────────────────────────────────────────────
BOROUGH_SHOP = os.path.join("data", "processed", "borough_shoplifting_trend.csv")
BOROUGH_DEP  = os.path.join("data", "processed", "borough_outliers_deprivation.csv")
SS_BOROUGH   = os.path.join("data", "processed", "ss_borough_full.csv")
STREET_PATH  = os.path.join("data", "processed", "street_clean.csv")   # fixed
OUT_DIR      = os.path.join("data", "processed")

# ── Vulnerability index weights ───────────────────────────────────
WEIGHTS = {
    "deprivation":        0.35,
    "shoplifting_trend":  0.30,
    "crime_dep_mismatch": 0.20,
    "policing_intensity": 0.15,
}

RISK_THRESHOLDS = {"higher": 60, "lower": 35}

# ── Borough centroids ─────────────────────────────────────────────
BOROUGH_CENTROIDS = {
    "Barking and Dagenham":   (51.5362,  0.0798),
    "Barnet":                  (51.6252, -0.1517),
    "Bexley":                  (51.4549,  0.1505),
    "Brent":                   (51.5588, -0.2817),
    "Bromley":                 (51.4039,  0.0198),
    "Camden":                  (51.5290, -0.1255),
    "City of London":          (51.5155, -0.0922),
    "Croydon":                 (51.3714, -0.0977),
    "Ealing":                  (51.5130, -0.3089),
    "Enfield":                 (51.6521, -0.0807),
    "Greenwich":               (51.4934,  0.0098),
    "Hackney":                 (51.5450, -0.0553),
    "Hammersmith and Fulham":  (51.4927, -0.2339),
    "Haringey":                (51.5906, -0.1119),
    "Harrow":                  (51.5836, -0.3464),
    "Havering":                (51.5779,  0.2120),
    "Hillingdon":              (51.5441, -0.4760),
    "Hounslow":                (51.4746, -0.3680),
    "Islington":               (51.5416, -0.1022),
    "Kensington and Chelsea":  (51.4991, -0.1938),
    "Kingston upon Thames":    (51.4123, -0.3007),
    "Lambeth":                 (51.4571, -0.1231),
    "Lewisham":                (51.4415, -0.0117),
    "Merton":                  (51.4014, -0.1958),
    "Newham":                  (51.5077,  0.0469),
    "Redbridge":               (51.5590,  0.0741),
    "Richmond upon Thames":    (51.4479, -0.3260),
    "Southwark":               (51.5035, -0.0804),
    "Sutton":                  (51.3618, -0.1945),
    "Tower Hamlets":           (51.5099, -0.0059),
    "Waltham Forest":          (51.5908, -0.0134),
    "Wandsworth":              (51.4567, -0.1919),
    "Westminster":             (51.4975, -0.1357),
}


# ── Helpers ───────────────────────────────────────────────────────

def minmax(series: pd.Series) -> pd.Series:
    lo, hi = series.min(), series.max()
    if hi == lo:
        return pd.Series(0.5, index=series.index)
    return (series - lo) / (hi - lo)


def load_shop() -> pd.DataFrame:
    """
    borough_shoplifting_trend.csv has one row per borough (not per year).
    Columns: borough, count_2023, count_2025, change_pct, avg_imd_decile.
    Guard against accidentally getting the multi-year version.
    """
    shop = pd.read_csv(BOROUGH_SHOP)

    # If the file has a 'year' column it's the wrong shape — aggregate down
    if "year" in shop.columns:
        shop = (
            shop.groupby("borough")
            .agg(
                count_2023=("count_2023", "first") if "count_2023" in shop.columns
                           else ("year", "count"),
                change_pct=("change_pct", "mean"),
                avg_imd_decile=("avg_imd_decile", "mean"),
            )
            .reset_index()
        )

    # If there's no change_pct column, derive it
    if "change_pct" not in shop.columns:
        if "count_2023" in shop.columns and "count_2025" in shop.columns:
            shop["change_pct"] = (
                (shop["count_2025"] - shop["count_2023"])
                / shop["count_2023"].replace(0, np.nan) * 100
            ).round(1)
        else:
            shop["change_pct"] = 0.0

    # Deduplicate — one row per borough
    shop = shop.drop_duplicates(subset=["borough"])
    return shop


# ── 1. Vulnerability index ────────────────────────────────────────

def build_vulnerability_index() -> pd.DataFrame:
    shop = load_shop()
    dep  = pd.read_csv(BOROUGH_DEP)
    ss   = pd.read_csv(SS_BOROUGH) if os.path.exists(SS_BOROUGH) else pd.DataFrame()

    # Component 1: deprivation (inverted — lower decile = more deprived = higher score)
    dep_c = dep[["borough", "avg_imd_decile", "residual"]].copy()
    dep_c = dep_c.drop_duplicates(subset=["borough"])
    dep_c["dep_score"] = 10 - dep_c["avg_imd_decile"].fillna(5)

    # Component 2: shoplifting trend
    shop_c = shop[["borough", "change_pct"]].copy()
    shop_c["shop_score"] = shop_c["change_pct"].fillna(0)

    # Component 3: crime-deprivation mismatch (positive residual = more crime than expected)
    dep_c["mismatch_score"] = dep_c["residual"].fillna(0)

    # Component 4: policing intensity adjusted for ineffectiveness
    if not ss.empty and "total_searches" in ss.columns and "arrest_rate" in ss.columns:
        ss_c = ss[["borough", "total_searches", "arrest_rate"]].copy()
        ss_c = ss_c.drop_duplicates(subset=["borough"])
        ss_c["arrest_rate"]    = ss_c["arrest_rate"].replace(0, 0.1)
        ss_c["policing_score"] = ss_c["total_searches"] / ss_c["arrest_rate"]
    else:
        print("  WARNING: stop and search data unavailable — policing component set to 0")
        ss_c = pd.DataFrame({
            "borough":        dep_c["borough"].unique(),
            "policing_score": 0,
        })

    # Join all components
    df = (
        dep_c[["borough", "dep_score", "mismatch_score"]]
        .merge(shop_c[["borough", "shop_score"]], on="borough", how="outer")
        .merge(ss_c[["borough", "policing_score"]],  on="borough", how="outer")
    )
    df.fillna(0, inplace=True)

    # Normalise to 0–1
    df["dep_norm"]      = minmax(df["dep_score"])
    df["shop_norm"]     = minmax(df["shop_score"])
    df["mismatch_norm"] = minmax(df["mismatch_score"])
    df["policing_norm"] = minmax(df["policing_score"])

    # Weighted composite score (0–100)
    df["vulnerability_score"] = (
        df["dep_norm"]      * WEIGHTS["deprivation"]        +
        df["shop_norm"]     * WEIGHTS["shoplifting_trend"]   +
        df["mismatch_norm"] * WEIGHTS["crime_dep_mismatch"]  +
        df["policing_norm"] * WEIGHTS["policing_intensity"]
    ) * 100
    df["vulnerability_score"] = df["vulnerability_score"].round(1)

    # Risk tiers
    def tier(score):
        if score >= RISK_THRESHOLDS["higher"]:
            return "Higher risk"
        if score <= RISK_THRESHOLDS["lower"]:
            return "Lower risk"
        return "Medium risk"

    df["risk_tier"] = df["vulnerability_score"].apply(tier)

    # Attach shoplifting change_pct for narrative use
    df = df.merge(shop[["borough", "change_pct"]], on="borough", how="left")

    # Attach centroids
    df["latitude"]  = df["borough"].map(
        lambda b: BOROUGH_CENTROIDS.get(b, (None, None))[0]
    )
    df["longitude"] = df["borough"].map(
        lambda b: BOROUGH_CENTROIDS.get(b, (None, None))[1]
    )

    # Drop rows with no centroid (non-London boroughs that crept in)
    df = df.dropna(subset=["latitude", "longitude"])

    keep = ["borough", "vulnerability_score", "risk_tier", "change_pct",
            "latitude", "longitude"]
    return df[[c for c in keep if c in df.columns]]


# ── 2. Crime trajectory ───────────────────────────────────────────

def build_crime_trajectory() -> pd.DataFrame:
    street = pd.read_csv(STREET_PATH)
    street["month"] = pd.to_datetime(street["month"])
    street["year"]  = street["month"].dt.year

    annual = (
        street.groupby(["crime_type", "year"])
        .size()
        .unstack(fill_value=0)
        .reset_index()
    )
    if 2023 not in annual.columns or 2025 not in annual.columns:
        raise ValueError("Data must cover both 2023 and 2025")

    annual["trend_pct"] = (
        (annual[2025] - annual[2023])
        / annual[2023].replace(0, np.nan) * 100
    ).round(1)

    context = {
        "Shoplifting":               ("Food inflation and household debt",
                                      "Retail crime legislation, neighbourhood policing"),
        "Theft from the person":     ("Tourism recovery and footfall",
                                      "Targeted policing in high-footfall areas"),
        "Drugs":                     ("Changed recording practice (Op Yamata)",
                                      "Recording consistency, drugs action plan"),
        "Possession of weapons":     ("Enforcement uplift post-Aug 2024",
                                      "Serious violence duty, knife crime strategy"),
        "Violence and sexual offences": ("Income and employment deprivation",
                                         "Violence reduction units, early intervention"),
        "Robbery":                   ("Structural deprivation, stable",
                                      "Neighbourhood policing, youth services"),
        "Burglary":                  ("Improved physical security, living environment",
                                      "Housing investment, smart home technology"),
        "Vehicle crime":             ("Anti-theft technology improvements",
                                      "Vehicle security standards"),
    }

    annual["key_driver"]   = annual["crime_type"].map(
        lambda x: context.get(x, ("Multiple factors", "General policing"))[0]
    )
    annual["policy_lever"] = annual["crime_type"].map(
        lambda x: context.get(x, ("Multiple factors", "General policing"))[1]
    )

    return annual[["crime_type", "trend_pct", "key_driver", "policy_lever"]]


# ── 3. Shoplifting scenarios ──────────────────────────────────────

def build_shoplifting_scenarios() -> pd.DataFrame:
    street = pd.read_csv(STREET_PATH)
    street["month"] = pd.to_datetime(street["month"])

    monthly = (
        street[street["crime_type"] == "Shoplifting"]
        .groupby("month").size()
        .reset_index(name="count")
        .sort_values("month")
    )

    last_val   = monthly.iloc[-1]["count"]
    last_month = monthly.iloc[-1]["month"]

    future_months = pd.date_range(
        start=last_month + pd.DateOffset(months=1),
        periods=12,
        freq="MS",
    )

    scenarios = pd.DataFrame({"month": future_months})
    scenarios["optimistic"]  = [round(last_val * (0.985 ** i)) for i in range(1, 13)]
    scenarios["central"]     = [round(last_val * (1.003 ** i)) for i in range(1, 13)]
    scenarios["pessimistic"] = [round(last_val * (1.018 ** i)) for i in range(1, 13)]

    return scenarios


# ── Main ──────────────────────────────────────────────────────────

def main():
    print("05_vulnerability_index.py")
    print("=" * 50)

    os.makedirs(OUT_DIR, exist_ok=True)

    print("Building vulnerability index...")
    vuln = build_vulnerability_index()
    vuln.to_csv(os.path.join(OUT_DIR, "borough_vulnerability.csv"), index=False)
    print(f"  ✓ {len(vuln)} boroughs scored")
    for t in ["Higher risk", "Medium risk", "Lower risk"]:
        print(f"  {t}: {(vuln['risk_tier'] == t).sum()}")

    print("Building crime trajectory...")
    traj = build_crime_trajectory()
    traj.to_csv(os.path.join(OUT_DIR, "crime_trajectory.csv"), index=False)
    print(f"  ✓ {len(traj)} crime types")

    print("Building shoplifting scenarios...")
    scen = build_shoplifting_scenarios()
    scen.to_csv(os.path.join(OUT_DIR, "shoplifting_scenarios.csv"), index=False)
    print(f"  ✓ {len(scen)} projected months")

    print(f"\n✓ Outlook outputs written to {OUT_DIR}")


if __name__ == "__main__":
    main()