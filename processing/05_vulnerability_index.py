"""
05_vulnerability_index.py
-------------------------
Combines deprivation, shoplifting trend, crime-deprivation mismatch,
and policing intensity into a composite borough vulnerability score.
Also builds the crime trajectory summary and shoplifting scenarios
for the 'Where is London Headed?' section.

Changes from original:
  - minmax() replaced with robust_normalise() using rank-based
    normalisation (resistant to outlier boroughs collapsing the scale)
  - build_weight_sensitivity() added: shows how top-5 borough rankings
    shift across four alternative weighting schemes
  - load_shop() rewritten: explicit column validation with clear error
    messages rather than silent broken aggregation
  - Outputs borough_weight_sensitivity.csv for dashboard expander

Fix (pipeline run): RISK_THRESHOLDS fixed thresholds (60/35) produced
a 26/7/0 split across 33 boroughs — 26 "Higher risk", 7 "Medium risk",
0 "Lower risk". Replaced with assign_risk_tiers_by_tertile() which
computes cut-points from the actual score distribution, guaranteeing a
roughly even 11/11/11 split regardless of how scores cluster. The
methodology expander in where_headed.py is updated to describe the
tertile method.

Outputs:
    data/processed/borough_vulnerability.csv
    data/processed/borough_weight_sensitivity.csv
    data/processed/crime_trajectory.csv
    data/processed/shoplifting_scenarios.csv

Run from project root:
    python processing/05_vulnerability_index.py
"""

import os
import numpy as np
import pandas as pd
from scipy.stats import rankdata

# ── Paths ─────────────────────────────────────────────────────────
BOROUGH_SHOP = os.path.join("data", "processed", "borough_shoplifting_trend.csv")
BOROUGH_DEP  = os.path.join("data", "processed", "borough_outliers_deprivation.csv")
SS_BOROUGH   = os.path.join("data", "processed", "ss_borough_full.csv")
STREET_PATH  = os.path.join("data", "processed", "street_clean.csv")
OUT_DIR      = os.path.join("data", "processed")

# ── Vulnerability index weights (base scenario) ───────────────────
WEIGHTS = {
    "deprivation":        0.35,
    "shoplifting_trend":  0.30,
    "crime_dep_mismatch": 0.20,
    "policing_intensity": 0.15,
}

# ── Alternative weighting scenarios for sensitivity analysis ──────
WEIGHT_SCENARIOS = {
    "Base": {
        "deprivation": 0.35, "shoplifting_trend": 0.30,
        "crime_dep_mismatch": 0.20, "policing_intensity": 0.15,
    },
    "Deprivation-heavy": {
        "deprivation": 0.55, "shoplifting_trend": 0.20,
        "crime_dep_mismatch": 0.15, "policing_intensity": 0.10,
    },
    "Crime-trend-heavy": {
        "deprivation": 0.20, "shoplifting_trend": 0.50,
        "crime_dep_mismatch": 0.20, "policing_intensity": 0.10,
    },
    "Equal": {
        "deprivation": 0.25, "shoplifting_trend": 0.25,
        "crime_dep_mismatch": 0.25, "policing_intensity": 0.25,
    },
}

# ── Borough centroids ─────────────────────────────────────────────
BOROUGH_CENTROIDS = {
    "Barking and Dagenham":   (51.5362,  0.0798),
    "Barnet":                  (51.6252, -0.1517),
    "Bexley":                  (51.4549,  0.1505),
    "Brent":                   (51.5588, -0.2817),
    "Bromley":                 (51.4039,  0.0198),
    "Camden":                  (51.5290, -0.1255),
    "City of London":          (51.5155, -0.0922),
    "Croydon":                 (51.3714, -0.0977),
    "Ealing":                  (51.5130, -0.3089),
    "Enfield":                 (51.6521, -0.0807),
    "Greenwich":               (51.4934,  0.0098),
    "Hackney":                 (51.5450, -0.0553),
    "Hammersmith and Fulham":  (51.4927, -0.2339),
    "Haringey":                (51.5906, -0.1119),
    "Harrow":                  (51.5836, -0.3464),
    "Havering":                (51.5779,  0.2120),
    "Hillingdon":              (51.5441, -0.4760),
    "Hounslow":                (51.4746, -0.3680),
    "Islington":               (51.5416, -0.1022),
    "Kensington and Chelsea":  (51.4991, -0.1938),
    "Kingston upon Thames":    (51.4123, -0.3007),
    "Lambeth":                 (51.4571, -0.1231),
    "Lewisham":                (51.4415, -0.0117),
    "Merton":                  (51.4014, -0.1958),
    "Newham":                  (51.5077,  0.0469),
    "Redbridge":               (51.5590,  0.0741),
    "Richmond upon Thames":    (51.4479, -0.3260),
    "Southwark":               (51.5035, -0.0804),
    "Sutton":                  (51.3618, -0.1945),
    "Tower Hamlets":           (51.5099, -0.0059),
    "Waltham Forest":          (51.5908, -0.0134),
    "Wandsworth":              (51.4567, -0.1919),
    "Westminster":             (51.4975, -0.1357),
}


# ── Normalisation ─────────────────────────────────────────────────

def robust_normalise(series: pd.Series) -> pd.Series:
    """
    Rank-based normalisation scaled to 0–1.

    Replaces minmax() which is sensitive to outlier boroughs: a single
    extreme value (e.g. City of London on crime rate) compresses all
    other values toward zero and strips meaningful variation from the
    composite score.

    Rank normalisation preserves ordinal relationships while being
    insensitive to the magnitude of outliers.
    """
    ranks = rankdata(series.fillna(series.median()), method="average")
    return pd.Series(ranks / len(ranks), index=series.index)


# ── Risk tier assignment (tertile-based) ──────────────────────────

def assign_risk_tiers_by_tertile(df: pd.DataFrame) -> pd.DataFrame:
    """
    Assign Higher/Medium/Lower risk tiers using tertile cut-points
    computed from the actual vulnerability score distribution.

    Fix (pipeline run): the previous fixed-threshold approach
    (RISK_THRESHOLDS = {"higher": 60, "lower": 35}) produced a 26/7/0
    split — nearly all boroughs "Higher risk", none "Lower risk" — making
    the map useless for differentiation.

    Tertile-based assignment guarantees a roughly even three-way split
    (11/11/11 across 33 boroughs) regardless of how scores cluster. The
    cut-points are printed so they can be cited in the methodology expander.

    Tiers:
        Higher risk  — vulnerability score >= 67th percentile
        Medium risk  — 33rd percentile < score < 67th percentile
        Lower risk   — vulnerability score <= 33rd percentile
    """
    p33 = df["vulnerability_score"].quantile(0.333)
    p67 = df["vulnerability_score"].quantile(0.667)

    print(f"  Risk tier tertile boundaries: "
          f"Lower ≤ {p33:.1f} < Medium < {p67:.1f} ≤ Higher")

    def _tier(score: float) -> str:
        if score >= p67:
            return "Higher risk"
        elif score <= p33:
            return "Lower risk"
        return "Medium risk"

    df = df.copy()
    df["risk_tier"] = df["vulnerability_score"].apply(_tier)
    return df


# ── Loader ────────────────────────────────────────────────────────

def load_shop() -> pd.DataFrame:
    """
    Load borough_shoplifting_trend.csv with explicit column validation.
    """
    shop = pd.read_csv(BOROUGH_SHOP)

    if "change_pct" not in shop.columns:
        if {"count_2023", "count_2025"}.issubset(shop.columns):
            print("  change_pct missing from shop file — deriving from count_2023/count_2025")
            shop["change_pct"] = (
                (shop["count_2025"] - shop["count_2023"])
                / shop["count_2023"].replace(0, np.nan) * 100
            ).round(1)
        else:
            raise ValueError(
                "borough_shoplifting_trend.csv is missing required columns.\n"
                f"Found: {list(shop.columns)}\n"
                "Expected: 'change_pct', or both 'count_2023' and 'count_2025'.\n"
                "Rerun processing/02_economic_analysis.py to regenerate."
            )

    if "borough" not in shop.columns:
        raise ValueError(
            "borough_shoplifting_trend.csv has no 'borough' column.\n"
            f"Found columns: {list(shop.columns)}"
        )

    n_before = len(shop)
    shop = shop.drop_duplicates(subset=["borough"])
    if len(shop) < n_before:
        print(f"  Dropped {n_before - len(shop)} duplicate borough rows from shop file")

    return shop


# ── 1. Vulnerability index ────────────────────────────────────────

def build_vulnerability_index() -> pd.DataFrame:
    shop = load_shop()
    dep  = pd.read_csv(BOROUGH_DEP)
    ss   = pd.read_csv(SS_BOROUGH) if os.path.exists(SS_BOROUGH) else pd.DataFrame()

    # Component 1: deprivation (inverted — lower decile = more deprived = higher score)
    dep_c = dep[["borough", "avg_imd_decile", "residual"]].copy()
    dep_c = dep_c.drop_duplicates(subset=["borough"])
    dep_c["dep_score"] = 10 - dep_c["avg_imd_decile"].fillna(5)

    # Component 2: shoplifting trend
    shop_c = shop[["borough", "change_pct"]].copy()
    shop_c["shop_score"] = shop_c["change_pct"].fillna(0)

    # Component 3: crime-deprivation mismatch (positive residual = more crime than expected)
    dep_c["mismatch_score"] = dep_c["residual"].fillna(0)

    # Component 4: policing intensity adjusted for ineffectiveness
    if not ss.empty and "total_searches" in ss.columns and "arrest_rate" in ss.columns:
        ss_c = ss[["borough", "total_searches", "arrest_rate"]].copy()
        ss_c = ss_c.drop_duplicates(subset=["borough"])
        ss_c["arrest_rate"]    = ss_c["arrest_rate"].replace(0, 0.1)
        ss_c["policing_score"] = ss_c["total_searches"] / ss_c["arrest_rate"]
    else:
        print("  WARNING: stop and search data unavailable — policing component set to 0")
        ss_c = pd.DataFrame({
            "borough":        dep_c["borough"].unique(),
            "policing_score": 0,
        })

    # Join all components
    df = (
        dep_c[["borough", "dep_score", "mismatch_score"]]
        .merge(shop_c[["borough", "shop_score"]], on="borough", how="outer")
        .merge(ss_c[["borough", "policing_score"]], on="borough", how="outer")
    )
    df.fillna(0, inplace=True)

    # Rank-based normalisation (robust to outlier boroughs)
    df["dep_norm"]      = robust_normalise(df["dep_score"])
    df["shop_norm"]     = robust_normalise(df["shop_score"])
    df["mismatch_norm"] = robust_normalise(df["mismatch_score"])
    df["policing_norm"] = robust_normalise(df["policing_score"])

    # Weighted composite score (0–100)
    df["vulnerability_score"] = (
        df["dep_norm"]      * WEIGHTS["deprivation"]        +
        df["shop_norm"]     * WEIGHTS["shoplifting_trend"]   +
        df["mismatch_norm"] * WEIGHTS["crime_dep_mismatch"]  +
        df["policing_norm"] * WEIGHTS["policing_intensity"]
    ) * 100
    df["vulnerability_score"] = df["vulnerability_score"].round(1)

    # Fix (pipeline run): replaced fixed thresholds with tertile assignment
    df = assign_risk_tiers_by_tertile(df)

    # Attach shoplifting change_pct for narrative use
    df = df.merge(shop[["borough", "change_pct"]], on="borough", how="left")

    # Attach centroids
    df["latitude"]  = df["borough"].map(
        lambda b: BOROUGH_CENTROIDS.get(b, (None, None))[0]
    )
    df["longitude"] = df["borough"].map(
        lambda b: BOROUGH_CENTROIDS.get(b, (None, None))[1]
    )

    # Drop rows with no centroid (non-London boroughs that crept in)
    df = df.dropna(subset=["latitude", "longitude"])

    keep = ["borough", "vulnerability_score", "risk_tier", "change_pct",
            "latitude", "longitude",
            # keep normalised components for sensitivity analysis
            "dep_norm", "shop_norm", "mismatch_norm", "policing_norm"]
    return df[[c for c in keep if c in df.columns]]


# ── 2. Weight sensitivity analysis ───────────────────────────────

def build_weight_sensitivity(vuln_df: pd.DataFrame) -> pd.DataFrame:
    """
    Show how borough rankings shift across alternative weighting schemes.
    """
    required = {"borough", "dep_norm", "shop_norm", "mismatch_norm", "policing_norm"}
    missing  = required - set(vuln_df.columns)
    if missing:
        raise ValueError(f"build_weight_sensitivity: missing columns {missing}")

    rows = []
    for scenario_name, weights in WEIGHT_SCENARIOS.items():
        score = (
            vuln_df["dep_norm"]      * weights["deprivation"]        +
            vuln_df["shop_norm"]     * weights["shoplifting_trend"]   +
            vuln_df["mismatch_norm"] * weights["crime_dep_mismatch"]  +
            vuln_df["policing_norm"] * weights["policing_intensity"]
        ) * 100
        ranked = score.rank(ascending=False, method="min").astype(int)
        for borough, rank, sc in zip(vuln_df["borough"], ranked, score):
            rows.append({
                "borough":  borough,
                "scenario": scenario_name,
                "rank":     int(rank),
                "score":    round(float(sc), 1),
            })

    long_df = pd.DataFrame(rows)

    top5_counts = (
        long_df[long_df["rank"] <= 5]
        .groupby("borough")
        .size()
        .rename("top5_count")
        .reset_index()
    )
    long_df = long_df.merge(top5_counts, on="borough", how="left")
    long_df["top5_count"] = long_df["top5_count"].fillna(0).astype(int)

    return long_df.sort_values(["scenario", "rank"])


# ── 3. Crime trajectory ───────────────────────────────────────────

def build_crime_trajectory() -> pd.DataFrame:
    street = pd.read_csv(STREET_PATH)
    street["month"] = pd.to_datetime(street["month"])
    street["year"]  = street["month"].dt.year

    annual = (
        street.groupby(["crime_type", "year"])
        .size()
        .unstack(fill_value=0)
        .reset_index()
    )
    if 2023 not in annual.columns or 2025 not in annual.columns:
        raise ValueError("Data must cover both 2023 and 2025")

    annual["trend_pct"] = (
        (annual[2025] - annual[2023])
        / annual[2023].replace(0, np.nan) * 100
    ).round(1)

    context = {
        "Shoplifting":               ("Food inflation and household debt",
                                      "Retail crime legislation, neighbourhood policing"),
        "Theft from the person":     ("Tourism recovery and footfall",
                                      "Targeted policing in high-footfall areas"),
        "Drugs":                     ("Changed recording practice (Op Yamata)",
                                      "Recording consistency, drugs action plan"),
        "Possession of weapons":     ("Enforcement uplift post-Aug 2024",
                                      "Serious violence duty, knife crime strategy"),
        "Violence and sexual offences": ("Income and employment deprivation",
                                         "Violence reduction units, early intervention"),
        "Robbery":                   ("Structural deprivation, stable",
                                      "Neighbourhood policing, youth services"),
        "Burglary":                  ("Improved physical security, living environment",
                                      "Housing investment, smart home technology"),
        "Vehicle crime":             ("Anti-theft technology improvements",
                                      "Vehicle security standards"),
    }

    annual["key_driver"]   = annual["crime_type"].map(
        lambda x: context.get(x, ("Multiple factors", "General policing"))[0]
    )
    annual["policy_lever"] = annual["crime_type"].map(
        lambda x: context.get(x, ("Multiple factors", "General policing"))[1]
    )

    return annual[["crime_type", "trend_pct", "key_driver", "policy_lever"]]


# ── 4. Shoplifting scenarios ──────────────────────────────────────

def build_shoplifting_scenarios() -> pd.DataFrame:
    street = pd.read_csv(STREET_PATH)
    street["month"] = pd.to_datetime(street["month"])

    monthly = (
        street[street["crime_type"] == "Shoplifting"]
        .groupby("month").size()
        .reset_index(name="count")
        .sort_values("month")
    )

    last_val   = monthly.iloc[-1]["count"]
    last_month = monthly.iloc[-1]["month"]

    future_months = pd.date_range(
        start=last_month + pd.DateOffset(months=1),
        periods=12,
        freq="MS",
    )

    scenarios = pd.DataFrame({"month": future_months})
    scenarios["optimistic"]  = [round(last_val * (0.985 ** i)) for i in range(1, 13)]
    scenarios["central"]     = [round(last_val * (1.003 ** i)) for i in range(1, 13)]
    scenarios["pessimistic"] = [round(last_val * (1.018 ** i)) for i in range(1, 13)]

    return scenarios


# ── Main ──────────────────────────────────────────────────────────

def main():
    print("05_vulnerability_index.py")
    print("=" * 50)

    os.makedirs(OUT_DIR, exist_ok=True)

    print("Building vulnerability index (rank-based normalisation)...")
    vuln = build_vulnerability_index()

    # Strip internal normalised columns before saving main output
    save_cols = ["borough", "vulnerability_score", "risk_tier", "change_pct",
                 "latitude", "longitude"]
    vuln_save = vuln[[c for c in save_cols if c in vuln.columns]]
    vuln_save.to_csv(os.path.join(OUT_DIR, "borough_vulnerability.csv"), index=False)
    print(f"  ✓ {len(vuln_save)} boroughs scored")
    print(f"  Higher risk: {(vuln_save['risk_tier'] == 'Higher risk').sum()}")
    print(f"  Medium risk: {(vuln_save['risk_tier'] == 'Medium risk').sum()}")
    print(f"  Lower risk:  {(vuln_save['risk_tier'] == 'Lower risk').sum()}")

    print("Building weight sensitivity analysis...")
    try:
        sensitivity = build_weight_sensitivity(vuln)
        sensitivity.to_csv(
            os.path.join(OUT_DIR, "borough_weight_sensitivity.csv"), index=False
        )
        print(f"  ✓ {len(sensitivity)} rows ({len(WEIGHT_SCENARIOS)} scenarios × {len(vuln)} boroughs)")

        robust = (
            sensitivity[sensitivity["top5_count"] == len(WEIGHT_SCENARIOS)]
            [["borough", "top5_count"]]
            .drop_duplicates()
        )
        if not robust.empty:
            print(f"  Boroughs in top 5 across ALL {len(WEIGHT_SCENARIOS)} weight scenarios:")
            for _, row in robust.iterrows():
                print(f"    {row['borough']}")
        else:
            partial = (
                sensitivity[sensitivity["top5_count"] >= 3]
                [["borough", "top5_count"]]
                .drop_duplicates()
                .sort_values("top5_count", ascending=False)
            )
            print(f"  No borough in top 5 across all scenarios.")
            if not partial.empty:
                print(f"  Boroughs in top 5 in 3+ scenarios:")
                for _, row in partial.iterrows():
                    print(f"    {row['borough']} ({int(row['top5_count'])}/{len(WEIGHT_SCENARIOS)} scenarios)")

    except Exception as e:
        print(f"  ERROR building weight sensitivity: {e}")

    print("Building crime trajectory...")
    traj = build_crime_trajectory()
    traj.to_csv(os.path.join(OUT_DIR, "crime_trajectory.csv"), index=False)
    print(f"  ✓ {len(traj)} crime types")

    print("Building shoplifting scenarios...")
    scen = build_shoplifting_scenarios()
    scen.to_csv(os.path.join(OUT_DIR, "shoplifting_scenarios.csv"), index=False)
    print(f"  ✓ {len(scen)} projected months")

    print(f"\n✓ Outlook outputs written to {OUT_DIR}")


if __name__ == "__main__":
    main()